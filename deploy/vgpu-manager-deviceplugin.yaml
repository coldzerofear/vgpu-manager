# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vgpu-manager-device-plugin
  namespace: kube-system
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vgpu-manager-device-plugin:role
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get","list","watch","update","patch"]
  - apiGroups: [""]
    resources: ["nodes/status"]
    verbs: ["patch"]
#  - apiGroups: [""]
#    resources: ["nodes/proxy"]
#    verbs: ["get"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["create","get","list","watch","update","patch","delete"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["get","update","patch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create","patch","update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vgpu-manager-device-plugin-as-role
subjects:
  - kind: ServiceAccount
    name: vgpu-manager-device-plugin
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: vgpu-manager-device-plugin:role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
data:
  nodeConfig.json: |
    [
      {
        "nodeName": "demo0",
        "cgroupDriver": "systemd",
        "deviceListStrategy": "envvar",
        "deviceSplitCount": 10,
        "deviceMemoryScaling": 1,
        "deviceMemoryFactor": 1,
        "deviceCoresScaling": 1,
        "excludeDevices": "0-1",
        "openKernelModules": false
      }
    ]
kind: ConfigMap
metadata:
  name: vgpu-manager-device-plugin-config
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: vgpu-manager-device-plugin
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: vgpu-manager-device-plugin
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      # This annotation is deprecated. Kept here for backward compatibility
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: vgpu-manager-device-plugin
    spec:
      tolerations:
        # This toleration is deprecated. Kept here for backward compatibility
        # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
      # Mark this pod as a critical add-on; when enabled, the critical add-on
      # scheduler reserves resources for critical add-on pods so that they can
      # be rescheduled after a failure.
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      priorityClassName: system-node-critical
      nodeSelector:
        vgpu-manager-enable: enable
      serviceAccount: vgpu-manager-device-plugin
      terminationGracePeriodSeconds: 30
      restartPolicy: Always
      hostNetwork: true
      hostPID: true
      initContainers:
        - name: install
          command: ["/bin/bash", "-c", "/scripts/install_files.sh"]
          image: coldzerofear/vgpu-manager:latest
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 10m
              memory: 15Mi
          securityContext:
            privileged: true
          volumeMounts:
            - name: manager
              mountPath: /etc/vgpu-manager
              mountPropagation: Bidirectional
      containers:
        - name: device-plugin
          image: coldzerofear/vgpu-manager:latest
          imagePullPolicy: IfNotPresent
          command:
            - deviceplugin
            - --device-split-count=10
            - --device-cores-scaling=1
            - --device-memory-scaling=1
            - --node-config-path=/config/nodeConfig.json
            - --open-kernel-modules=false
            - --v=4
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
#            - name: CGROUP_DRIVER
#              value: systemd
            - name: HOST_MANAGER_DIR
              value: /etc/vgpu-manager
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: NVIDIA_MIG_MONITOR_DEVICES
              value: all
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: utility,compute
          securityContext:
            #privileged: true
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
              add: ["SYS_ADMIN"]
          resources: {}
          volumeMounts:
            - name: kubelet
              mountPath: /var/lib/kubelet
            - name: manager
              mountPath: /etc/vgpu-manager
            - name: cgroup
              mountPath: /sys/fs/cgroup
            - name: node-config
              mountPath: /config
        - name: device-monitor
          image: coldzerofear/vgpu-manager:latest
          imagePullPolicy: IfNotPresent
          command:
            - monitor
            - --server-bind-port=3456
            - --node-config-path=/config/nodeConfig.json
            - --v=4
          ports:
            - name: http
              containerPort: 3456
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
#            - name: CGROUP_DRIVER
#              value: systemd
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: NVIDIA_MIG_MONITOR_DEVICES
              value: all
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: utility,compute
          securityContext:
            #privileged: true
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
              add: ["SYS_ADMIN"]
          resources: {}
          livenessProbe:
            httpGet:
              path: /healthz
              port: 3456
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 3456
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
          volumeMounts:
            - name: kubelet
              mountPath: /var/lib/kubelet
            - name: manager
              mountPath: /etc/vgpu-manager
            - name: cgroup
              mountPath: /sys/fs/cgroup
            - name: node-config
              mountPath: /config
      volumes:
        - name: kubelet
          hostPath:
            path: /var/lib/kubelet
        - name: manager
          hostPath:
            path: /etc/vgpu-manager
            type: DirectoryOrCreate
        - name: cgroup
          hostPath:
            path: /sys/fs/cgroup
        - name: node-config
          configMap:
            name: vgpu-manager-device-plugin-config
---
apiVersion: v1
kind: Service
metadata:
  name: vgpu-manager-monitor
  namespace: kube-system
spec:
  ports:
    - name: http
      port: 3456
      protocol: TCP
      targetPort: http
  selector:
    app: vgpu-manager-device-plugin
  type: ClusterIP